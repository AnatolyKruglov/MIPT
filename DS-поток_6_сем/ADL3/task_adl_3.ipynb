{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b8eec68-75eb-4e3a-8ca7-922aaea04e86",
      "metadata": {},
      "source": [
        "# DS-поток, весна 2025\n",
        "## Задание ADL.3\n",
        "### LLM, Alignment.\n",
        "\n",
        "**Правила:**\n",
        "\n",
        "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
        "* Выполненную работу нужно отправить телеграм-боту `@miptstats_ds24_bot`. Для начала работы с ботом каждый раз отправляйте `/start`. Дождитесь подтверждения от бота, что он принял файл. Если подтверждения нет, то что-то не так. **Работы, присланные иным способом, не принимаются.**\n",
        "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
        "* Прислать нужно **ноутбук в формате `ipynb`**.\n",
        "* Следите за размером файлов. **Бот не может принимать файлы весом более 20 Мб.** Если файл получается больше, заранее разделите его на несколько.\n",
        "* Выполнять задание необходимо полностью самостоятельно. **При обнаружении списывания все участники списывания получат штраф.**\n",
        "* Решения, размещенные на каких-либо интернет-ресурсах, не принимаются. Кроме того, публикация решения в открытом доступе может быть приравнена к предоставлении возможности списать.\n",
        "* Для выполнения задания используйте этот ноутбук в качестве основы, ничего не удаляя из него. Можно добавлять необходимое количество ячеек.\n",
        "* Комментарии к решению пишите в markdown-ячейках.\n",
        "* Выполнение задания (ход решения, выводы и пр.) должно быть осуществлено на русском языке.\n",
        "* Если код будет не понятен проверяющему, оценка может быть снижена.\n",
        "* Никакой код из данного задания при проверке запускаться не будет. *Если код студента не выполнен, недописан и т.д., то он не оценивается.*\n",
        "* В каждой задаче не забывайте делать **пояснения и выводы**.\n",
        "* **Код из рассказанных на занятиях ноутбуков** можно использовать без ограничений.\n",
        "\n",
        "**Баллы за задание:**\n",
        "\n",
        "* Задача 1 &mdash; 40 баллов;\n",
        "* Задача 2 &mdash; 60 баллов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4476836d-8b60-4907-a1dc-fdf214f6edcf",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d67c56c-5b4f-4baf-bfa3-19d3e2c9b008",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import torch\n",
        "import unsloth\n",
        "\n",
        "from trl import (\n",
        "    SFTTrainer,\n",
        "    DataCollatorForCompletionOnlyLM\n",
        ")\n",
        "\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer, \n",
        "    LlamaTokenizerFast\n",
        ")\n",
        "from unsloth import (\n",
        "    FastLanguageModel,\n",
        "    is_bfloat16_supported\n",
        ")\n",
        "from unsloth.chat_templates import get_chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78fc6392-5416-4c49-8426-870582dc468e",
      "metadata": {},
      "source": [
        "---\n",
        "### Задача 1. \n",
        "\n",
        "В этой задаче вам предложено провести сравнительное исследование, оценив эффективность нескольких LLM в различных форматах: zero-/few-shot для задачи анализа тональности текста на датасете отзывов IMDB.\n",
        "\n",
        "---\n",
        "**Форматы zero-/few-shot**\n",
        "\n",
        "Современные большие языковые модели (LLM), обученные следовать инструкциям, позволяют решать задачи классификации, если правильно оформить запрос к ним. Подготовку надежной инструкции для решения задачи иногда еще называют промптингом. Подробнее можно почитать <a href=\"https://docs.mistral.ai/guides/prompting_capabilities/\" target=\"_top\">здесь</a> или <a href=\"https://huggingface.co/docs/transformers//tasks/prompting\" target=\"_top\">здесь</a>, или самостоятельно обсудить с LLM!\n",
        "\n",
        "\n",
        "Рассмотрим два довольно популярных подхода.\n",
        "* *Zero-Shot* \\\n",
        "  Модель выполняет задачу, опираясь только на инструкцию, без дополнительных примеров. Допустим вы можете сформулировать запрос к модели так:\n",
        "  ```\n",
        "  Определи, POSITIVE или NEGATIVE тон у этого текста: {текст}.\n",
        "  ```\n",
        "  <br>\n",
        "* *Few-Shot* \\\n",
        "  Модель получает $k$ **примеров вместе с ответом** перед основным запросом. Это позволяет познакомить модель, например, с форматом ответа и показать какие-то паттерны через демонстрацию. \\\n",
        "  Рассмотрим возможный `2-shot` формат. Он предполагает демонстрацию на **двух примерах вместе с референсным ответом**. Третьим запросом будет идти целевой:\n",
        "\n",
        "  ```\n",
        "  Пример 1: \"Фильм ужасен...\" → NEGATIVE\n",
        "  Пример 2: \"Это шедевр!\" → POSITIVE\n",
        "  Задача: Определи тональность для \"{текст}\".\n",
        "  ```\n",
        "\n",
        "Обратим внимание, что Few-Shot-подход можно реализовать разными способами в зависимости от API или чат-интерфейса модели.\n",
        "\n",
        "1. Использовать системный промпт и указать инструкцию **вместе с примерами** прямо там.\n",
        "\n",
        "```\n",
        "system_prompt = \"\"\"\n",
        "Ты классифицируешь тональность текста. Вот примеры:  \n",
        "- \"Фильм ужасен...\"\\nОтвет: NEGATIVE  \n",
        "- \"Это шедевр!\"\\nОтвет: POSITIVE  \n",
        "Отвечай только метками POSITIVE/NEGATIVE.  \n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "В таком случае в `user`-реплике будет содержаться только текст целевого отзыва. При этом можно поступить иначе и использовать, например, стандартный системный промпт, рекомендованный разработчиками модели. Тогда инструкцию и few-shot примеры можно поместить сразу в `user`-реплику.\n",
        "\n",
        "\n",
        "2. Используя чат-шаблон (chat-template). Тогда k-shot вариант можно реализовать следующим образом:\n",
        "\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"Ты классифицируешь тональность текста.\"},  # Общая инструкция в сис. промпте, но можно и в user-реплику!\n",
        "{\"role\": \"user\", \"content\": \"Отзыв: 'Сюжет скучный...'\\nОтвет:\"},         # Пример 1\n",
        "{\"role\": \"assistant\", \"content\": \"NEGATIVE\"},                             # Важно: ответ модели заполнен нами самостоятельно! Его можно взять из train-сета\n",
        "...\n",
        "{\"role\": \"user\", \"content\": \"Отзыв: 'Лучший фильм года!'\\nОтвет:\"},       # Пример k\n",
        "{\"role\": \"assistant\", \"content\": \"POSITIVE\"},                             # Ответ для примера k\n",
        "{\"role\": \"user\", \"content\": \"Отзыв: '{review}'\\nОтвет:\"}                  # Тут расположен целевой пример для прогона\n",
        "```\n",
        "В данном случае $k$ примеров, которые можно взять из обучающей выборки, вместе с ответами и примером для инференса принудительно помещены в историю диалога. Модель, генерируя след. реплику, выдаст нам ответ на задачу, а примеры в контексте позволят лучше следовать формату ответа и потенциально повысят итоговое качество.\n",
        "\n",
        "Выбор стратегии, вообще говоря, зависит от пользователя и ограничений модели. Например, некоторые модели могут не поддерживать системный промпт.\n",
        "\n",
        "---\n",
        "\n",
        "Итак, вам требуется применить **две** open-source LLM для задачи sentiment analysis на датасете IMDB и сравнить их производительность в режимах zero- и few-shot на **валидационной подвыборке размера 2k сэмплов.**\n",
        "\n",
        "Для few-shot используйте $k=5$ примеров, которые возьмите случайным образом из обучающей выборки. Выбор конкретного формата из описанных выше остается на вашей стороне :)\n",
        "\n",
        "\n",
        "**Подумайте**, должны ли few-shot примеры быть сбалансированы, то есть содержать приблизительно одинаковое кол-во позитивных и негативных примеров? Какие стратегии выбора фьюшот примеров вы можете предложить?\n",
        "\n",
        "В качестве LLM используйте `Qwen/Qwen2.5-3B` и `unsloth/Llama-3.2-1B-Instruct` с HuggingFace. Попробуйте **поэкспериментировать с различными вариантами промпта**. Можете взять за основу пример с семинара и воспользоваться ИИ-инструментами для его усовершенствования. Сравните результат с моделью на базе RNN, с которой вы имели дело ранее.\n",
        "\n",
        "Представьте результат в виде аккуратной таблицы, в которой укажите модель вместе с используемым форматом (zero-/few-shot), а также итоговую точность (accuracy). Сделайте выводы."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c6458a-1772-4693-aa32-c0108e3ebb6a",
      "metadata": {},
      "source": [
        "*Замечание* \\\n",
        "*Для инференса рекомендуется использовать `vLLM.` Метод `.chat` может обрабатывать батч запросов, что позволит ускорить вычисления и повысить утилизацию GPU.*\n",
        "```\n",
        "batch_indices = range(batch_start, batch_start + batch_size)\n",
        "batch_data = data.select(batch_indices)\n",
        "conversations = [[{\"role\": \"user\", \"content\": format_text_request(review)}] for review in batch_data[\"review\"]]\n",
        "outputs = model.chat(conversations, ...)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0609b35f-b465-4655-ad53-761ee47a1c94",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = load_dataset(\n",
        "    \"scikit-learn/imdb\", split=\"train\"\n",
        ").train_test_split(test_size=0.2, seed=42)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0852eabe-db26-4568-9367-41a57f6a07bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_subsample = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0330a1e8-586d-4519-a576-a38fa4ed91ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02fa8c1a-f2ca-4a35-a303-d0f6c19f8ce4",
      "metadata": {},
      "source": [
        "**Вывод**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0cf140c-1233-4d24-bb27-8cfb32918779",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea1e5fe8-e32f-411a-bc1e-90849db1f270",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "---\n",
        "### Задача 2.\n",
        "\n",
        "В этой задаче вы разберете пример Supervised Fine-Tuning (SFT) для новой русскоязычной базовой модели, недавно выпущенной Яндексом. Подробнее о модели можно прочесть [здесь](https://habr.com/ru/companies/yandex/articles/895428/) и [здесь](https://habr.com/ru/companies/yandex/articles/885218/).\n",
        "\n",
        "SFT (Supervised Fine-Tuning) в данном случае — это этап дообучения предварительно обученной языковой модели на большом наборе инструктивных данных, называемый также Instruction Finetuning. Мы будем учить LLM следовать указаниям пользователя, что критически важно для создания современных ассистентов и чат-ботов. На этапе SFT модель адптируется к конкретным задачам: генеративные ответы на вопросы, диалог с пользователям, задачи классификации и генерации кода, и т. д.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3727d80f-92e5-42bd-9183-ea34c642c126",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE8oCYXeNcZU",
        "outputId": "45532639-9116-4ab3-ca13-a718baee404f"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"yandex/YandexGPT-5-Lite-8B-pretrain\"\n",
        "# Будем работать с небольшим контекстом в 2k токенов\n",
        "# Современные модели обычно имеют контекст в 32k-128k токенов\n",
        "# Некоторые техники позволяют добиться контекста ~1m токенов.\n",
        "MAX_SEQ_LENGTH = 2048"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c6ac061-6fa7-4978-9963-dccac85ec2b7",
      "metadata": {
        "id": "ItgGCjC2kiGJ"
      },
      "source": [
        "Для начала загрузим датасет. Это смешанный набор инструктивных данных из разных источников с полезной мета-информацией. На этом датасете обучаются в том числе модели opensource-проекта [Сайга](t.me/saiga_igusev_bot)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edeff110-87a0-4c1c-8a41-ba76b60fa83f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "52b73b100cf149ceb02f4577ace1fda2",
            "d23bb2957617464883dba52e02cb4ed3",
            "c041ed03be89407d84b3d715abcae885",
            "327b10137ac2402699d0d3ad67be5805",
            "02ee77021b5849c48916c397a9113950",
            "da24cc4216334e46a8e06a32228f6789",
            "23fff407c1704e9fa56fee1c5cc2d8a7",
            "1586b5dd1e684c01be590c8670a90b8f",
            "4b3cad9ad0504fc384a5f7dd8e200233",
            "4f0b33facc174fcab2306fe624c4f12a",
            "1bfbb5ccefbf4e1fac924c1a32d2ca3b",
            "e69ae4def5914c0d88fded1708fa2673",
            "192c26b3e07e472897ba1e40cb580764",
            "b75d52c8806b4626aa5a3de54404f9c5",
            "4bab6bafb1d9402e89b1eaa2bd6c3660",
            "045e409231774e908ee602c32760ec09",
            "6604096421a44b2d9a5af53b88962617",
            "6c647d4187634b6bbd6aaf55d3eecfe0",
            "3c2aa15722c043d48b83c83082b0eaac",
            "17cac7e81e554455be5d2437fc592ccf",
            "d2144c540b3f4b7c89f2add54688a074",
            "c2271cf10acc44a380f99d461f27b032",
            "fc0090dd4b7d4c8b9f866971b0d8579b",
            "7b5ebb3531e14f6ab6d71d849d0a5fc2",
            "6e1fcf2b45964c47aa9a9f0805f6cfc7",
            "1c7d1226c5434a51aa9951155436cc53",
            "7b813e2e43a64459bbc96f48d01aeb51",
            "65cbbedfd4b4463f82805fd7861fe68c",
            "4ab35de11fdd4604a07b7d64b072c3e6",
            "2b33090f9edc469db54aa69facc690a5",
            "8a4f0d7db33942c292f35ab62727a9c3",
            "4d0e9df8cbc4424a9e3c9fb3ff2ac246",
            "c57cf166e6d7450a880602dc144663f1"
          ]
        },
        "id": "D8X6rmoDRU_y",
        "outputId": "6bbcdbb7-2852-47e3-c183-1f8dcf8b6e90"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"IlyaGusev/saiga_scored\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69bfc5b7-baaf-4e86-af5a-cd63ccbfbc08",
      "metadata": {
        "id": "-E15blfblFhq"
      },
      "source": [
        "Датасет достаточно большого размера, содержит информацию об источнике, языке диалога, а также полезные признаки в виде оценки тематики / сложности сэмпла, полученные с помощью более производительных LLM [(Sonnet, Opus)](https://habr.com/ru/companies/bothub/articles/823580/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb0bd165-9233-4f6a-85d4-12d651d77cc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUo-aFBeRX28",
        "outputId": "7ea45ab6-8de7-4ed7-ee6e-3448e741f3be"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "393556bd-7f30-4c43-b06d-5c8d70732421",
      "metadata": {
        "id": "vbcCdZFulZQh"
      },
      "source": [
        "Посмотрим на какой-нибудь пример."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d40195-c1e8-45ab-b402-36a64f819b88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtfJ7wk7Uy6F",
        "outputId": "57c6b9b8-7d1e-464c-b19a-e8755a11a197"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][1800]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b41f5a-94ba-4970-b2ee-849f8a035b9d",
      "metadata": {},
      "source": [
        "Вам предлагается поработать с подвыборкой из этого сета. Для этого проведите базовую аналитку: посмотрите на распр. по языкам, тематикам и сложности данных. Посчитайте базовые статистики (среднее, минимум и максимум) для реплик модели / длины диалога.\n",
        "Учтите, что в большей степени нас интересуют инструкции и ответы на русском языке, небольшую аналитику можно провести для него отдельно. Не забывайте про аккуратность графиков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "469980fd-2393-4135-a949-b56f138dcb9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eae28b4-29e3-4e97-951b-04fb687609f8",
      "metadata": {},
      "source": [
        "Исходя из проведенной аналитики попробуйте через различные эвристики отфильтровать датасет. При этом для простоты решения задачи важно будет оставить только одношаговые диалоги. В итоге будет достаточно 2k-3k качественных сэмплов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af10d440-9fab-40c9-8e42-1c32b294ce53",
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_data = dataset[\"train\"].filter(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea6a68f-db8b-4425-8829-c93c171f3866",
      "metadata": {},
      "source": [
        "Как вы могли заметить вместо привычного `assistant` в списке сообщений `messages` встречается роль `bot`. Давайте использовать более классический вариант."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9e7bda-6719-480a-99b9-96225d78c4ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rename_bot_in_messages(example):\n",
        "    ...\n",
        "    return example\n",
        "\n",
        "filtered_data = filtered_data.map(rename_bot_in_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0388eb6b-6824-447f-a7a7-e49e196762a6",
      "metadata": {},
      "source": [
        "Теперь разберемся с токенизатором."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b7ca75f-b479-498d-851d-f61890b2fd94",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = LlamaTokenizerFast.from_pretrained(MODEL_NAME, legacy=False)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a1d7466-4fc2-4830-bf13-c82f4ecaabc1",
      "metadata": {},
      "source": [
        "С 128000 по 129024 токен находятся зарезервированные спец. токены. Они предварительно добавлены в токенизатор и эмбеддинг слой модели для удобства, но не участвовали в претрейне. Например спец. токены пригодятся нам, чтобы грамотно обучить модель следовать чат-шаблону. Можно использовать токены с 128000 включительно, поменяв представление `[SPEC_TOKEN_{IDX}]` из токенизатора на наболее привычный `<|start_header_id|>` и т. п.\n",
        "\n",
        "**Важно:** Мы будем делать LoRA-обучение, исходные эмбеддинги трогать не будем, поэтому в случае добавления спец. токенов они останутся необучаемыми, что будет не совсем корректно. Поэтому в данном случае мы все так же будем использовать условный `<|start_header_id|>`, но токенизироваться он будет не в один спец. токен, а в несколько простых."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a038f0-f098-476d-ac0b-4bb290e1451d",
      "metadata": {
        "id": "UYz_3zkDprDm"
      },
      "source": [
        "Будем использовать chat-template от серии моделей `llama-3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "923a3970-6ca4-457b-967a-d00c938b5664",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgQpMkQspwSf",
        "outputId": "0ac3012e-26ac-4df0-e425-6b5042246f6d"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3\",\n",
        "    # system_message= есть возможность задать системное сообщение\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d6cdf57-854a-4417-ab4a-c3a5a06b0fc9",
      "metadata": {
        "id": "AAlEYcoMqQEy"
      },
      "source": [
        "У нашего токенизатора появился чат-шаблон! Посмотрим на него. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d0186f-f32e-4d87-8203-92d3174526d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "03BPPTZCqTzc",
        "outputId": "cc639e29-cd0e-413e-dc86-d05d3d088b05"
      },
      "outputs": [],
      "source": [
        "tokenizer.chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0be3bb9-d3a0-46fe-9178-f9e0d7c3d4a5",
      "metadata": {},
      "source": [
        "Еще раз обратим внимание, `<|start_header_id|>` и похожие токены участвуют в шаблоне, но они НЕ добавлены как спец. токены, поэтому токенизируются в несколько токенов, как обычный текст."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81560db0-0bd5-4bea-a93f-f5486fe7896a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puams6Bdqd6b",
        "outputId": "8157620c-2e47-435c-8b35-36b90f675415"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9e320d-0e8c-424e-90b3-a0be5c27c6cc",
      "metadata": {
        "id": "_4ULLlNKq1wN"
      },
      "source": [
        "Посмотрим на пример применения чат-шаблона. Обучать модель будем предсказывать только `assistant`-реплики модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f57c2b-8e94-4174-b3f9-ce18d769f654",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "jgJouROuqwZ3",
        "outputId": "a33b7cbc-a133-42cc-c9dd-7ba1076af71b"
      },
      "outputs": [],
      "source": [
        "chat_template_sample = tokenizer.apply_chat_template(filtered_data[1800][\"messages\"], tokenize=False)\n",
        "chat_template_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e578148-c16e-40be-a5b4-aaed490eecb4",
      "metadata": {
        "id": "Gv_rWWEkra5r"
      },
      "source": [
        "Теперь загрузим модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4345487-d5d9-4c0b-9721-7fe4c818edb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "d393b8d8deae42c385370293b503941b",
            "a3c5dcc4b664415f9daeee61134912de",
            "12bf92a4505e4a49afccf92023dfb9a6",
            "0cab8fd670244e56b54e5b8d720dab5a",
            "a347eb5631a44b2dadbb973f0f17309b",
            "972e03912cae4098b69f6b7fc110a4d4",
            "dce1f40dc6fc46f9a341918810de1f0e",
            "576d546785ed4ee69b9bef76fd5e7025",
            "3a35a64b7f79455ca5a27685c97cf7b1",
            "fa6e5a70358d4e759172577fdec579bf",
            "6b4a326572714fee99ddaac21cda39a5",
            "1c5d8763fbb347fc87c275f0fc8488e0",
            "2e92a4aa96e44fef8ea5b19a96ef2b6c",
            "1943b92c29714d869bf72c5b55955075",
            "3f760d42dcba4f569ac0d1a2e8d0d5ac",
            "20c473a12f6e45b19e99941ca2381e4f",
            "8970f707f73b4ef5a011a05605fae4b1",
            "1f19e5f8166946cdb52c60a726080abf",
            "fd5eb3165712401c8df88e807c42058f",
            "0dfa404be7d147b192793b06e6938a92",
            "f6cba19cb1f24b14994de576ab68eb4d",
            "90c60647d7464bd2b7a9e8820a07a014",
            "b9a842756d9540fb90a995484366a233",
            "0f23f9a8b8634720b140f51b766edd3b",
            "01958bde424949eb8b935fe698f4315c",
            "6cb6c842e889449091d4d28306db01b2",
            "bc989619fe924dcd861bebf792ad5d0d",
            "6e17798fa8484d0483346ed12bd0ebcf",
            "411b9b3aebcf464abfa06c6665c307dd",
            "70c39647536643308b4dd1be3a8c8173",
            "fb723a45d75442708b6a3b1369610354",
            "d6068314e93a41689af0ab7016c11f12",
            "6d2647d1a1a5432689d5ea267d24b2c1",
            "471d0dcf66654134b2b9d61d9520eb67",
            "f774ac6c894b4460ab598f87ac50bd45",
            "26eb9c394fcd4ba0840989f9f5d1778d",
            "6e941ffdae004894bdeff1c1343d4b09",
            "1741b7a4cd7d42d49bed3fe36a4098cd",
            "0f3d3e46bd61459d8661b5ebfe3ee44b",
            "24859356d91b491a9e1f4d4b73c48f40",
            "df34d0e8d46841c990d66c3c673e3a70",
            "9d5f3153368b4b07ab323230ff907805",
            "ee2a1fabd33c4404af1358ef78b8f61d",
            "72f7e5ebd906468389cc21e3756cb537",
            "df9791705f5d4f7e82d1fa12aca2d96d",
            "0d5c10779ab24e0a86a37d6770878782",
            "b93171d27b8543be8c37013120f8d70d",
            "9dfc706b624d446d9c490354a52598c3",
            "8cce2b9cfa9340bcaaf845a61005d5f2",
            "65da755ce6f54eb099b4a893e7e4c19b",
            "7ccd670ea7bf47da9ec5e4ebf632a986",
            "8e6611c35161405ea87a257948f316ad",
            "ffa9e75a164c469796c80fa09f997f54",
            "0fb75075f12a4d5b9ece4953a7a8782e",
            "8b1e3cc738024a348a95c5464b28beda",
            "f4f7035192524190ab387e38e0a9d622",
            "6eac960889c944dea7d4d2562bc286d0",
            "0e1832dfcec54f179df921787ce0ef60",
            "6cc34595863743da82f1c5d69c9b5e25",
            "80ed1aba91154437b168c6a4e5bdfc4c",
            "0f3c9d745ab2498e8b92f575fd0aaa94",
            "706a04c29d5040e1a700c3fbb5427ba1",
            "18970f94c6ef473e90b29dcebf835c39",
            "4b3a1c378dd9483081b3407474cc9317",
            "ed59736515ed4aa49c774b576b7d3b46",
            "da6803f3a88e4af885e56b0025a316a1",
            "b401fc11052e4175896f2d279e670d1e",
            "ad4279e8c5464c6d867bc1af0f4bdacd",
            "75bafa6c8dee4f429e71501378b8e514",
            "e1a3aea27c8e4680b6e72bcf1d289a7d",
            "b007992b07564c3e82133eca6b2eee77",
            "0f29dda57c244b68b6a1fa1f7aa30be0",
            "af9e680a4e1449338eec45df26875395",
            "783a7b9c85da4eeaa2da5e7e418ef802",
            "80a8f52b4ba74dcca4c3e43092312128",
            "468bb3f9dd1b4b13ba7c563a4024eef5",
            "412256f6b2794d1fa33682a9c2fb0c27",
            "a841e896c6cd47b589a8d4fa1cbe3f2f",
            "53d7204d12bb4ba98d71d3a038c84ab7",
            "7980726d378a43b48a01071e6ddad95e",
            "5a005e7ca8564a2f931445ce564ea678",
            "d4fe1ca871ae46f6a900fc453662a2d8",
            "00d2009ca9144489b31453ec27945383",
            "c673f7c37462423fb37da9e72622868b",
            "18425471bd264ac2bc02ea71a666ac3a",
            "d4e1cf037a5d4ef9ae20fbf370d41673",
            "ef20685a0fcf424fa9f423ee7d026fa8",
            "2f518a30862642a6824cea2e1671217f"
          ]
        },
        "id": "Pnq_StWWrYHC",
        "outputId": "a730cf43-3be0-432a-d249-9af8b9ea7eb3"
      },
      "outputs": [],
      "source": [
        "# Будем использовать квантизацию в 4бита, подробнее, что это значит\n",
        "# мы узнаем на след. занятии\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Загрузка модели через знакомый метод .from_pretrained\n",
        "model, unsloth_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e765aea-93cf-4bf6-8f19-495e95abcb5a",
      "metadata": {
        "id": "rboVwUcyriTq"
      },
      "source": [
        "Важно помнить, что это пока базовая модель, она просто продолжает текст!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebe1ad71-a483-40fd-bc2f-4d6b8579a260",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "P70j7w8mrfPr",
        "outputId": "3ac8afb2-1bcb-4a7f-c061-4bfaf09d84d5"
      },
      "outputs": [],
      "source": [
        "prompt = \"План изучения машинного обучения:\"\n",
        "device = torch.device(\"cuda:0\")\n",
        "tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "# Посмотрим на генерацию\n",
        "outputs = model.generate(**tokenized_prompt,\n",
        "                         do_sample=True,\n",
        "                         temperature=0.3,\n",
        "                         max_new_tokens=128,\n",
        "                         # Используем KV-cache, а не пересчитываем векторы key/value\n",
        "                         use_cache=True)\n",
        "tokenizer.batch_decode(outputs)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f9f607-6118-413a-b0a2-4b0728f3c616",
      "metadata": {
        "id": "ilyKIxpHr3wt"
      },
      "source": [
        "Теперь добавим обучаемые LoRA-адаптеры. Полноценный SFT в рамках Colab'a будет сделать сложно, поэтому обойдемся peft-методом, функционал есть прямо в `unsloth`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9863e24-3822-4164-a9c1-236b6ce8b673",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0j3nJcCr76t",
        "outputId": "45c86ae9-cb60-4cf4-a14c-520c4deb646e"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    # Знакомые гиперпараметры LoRA\n",
        "    r=...,\n",
        "    target_modules=[\n",
        "        \"q_proj\",    # Attention\n",
        "        \"k_proj\",    # Attention\n",
        "        \"v_proj\",    # Attention\n",
        "        \"o_proj\",    # Attention\n",
        "        \"gate_proj\", # MLP\n",
        "        \"up_proj\",   # MLP\n",
        "        \"down_proj\", # MLP\n",
        "    ],\n",
        "    lora_alpha=...,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\", # Подробнее на след. занятии\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # Без rank stabilized LoRA\n",
        "    loftq_config=None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf982fd-dea7-428e-b214-82aa0b5bb6f0",
      "metadata": {
        "id": "Ap1rsur1V9oy"
      },
      "source": [
        "Напишем функцию для применения чат-шаблона к каждому сэмплу. Учтите, что токенизировать текст на этом этапе не стоит, добавлять спец. токены и `generation_prompt` тоже. Важно грамотно указать аргументы `.apply_chat_template(...)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39de41c2-fdb3-40da-80b2-765f4b9c4370",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b5aae318b3e14dbf8ac83227a40281dd",
            "6d556dd71ac045f48ffe15a84eef3a63",
            "a6015dbfc5284148aea242bddb1d2d86",
            "08d9ec7327414cd587460f20cf0c8009",
            "aa73314ccafe488a8485140994591712",
            "cb55033d9d9f485e8c2d0fca29d47819",
            "1a5189a149d2489687079f5b7ee72735",
            "495bb8345ba947d59100506536ffe9cd",
            "3ea1bc45c3b34b189a1507a659cc15ff",
            "11874eb9e2a947fcb93c4002cf3bae1b",
            "8272d81e677e4be38e13c02a81648959"
          ]
        },
        "id": "Bab4dBpss3I6",
        "outputId": "99cdae3d-f984-44a5-b4fa-115bd39a7d66"
      },
      "outputs": [],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = ...\n",
        "    return {\n",
        "        # Отформатированные тексты сообщений для последующей токенизации\n",
        "        \"text\" : texts\n",
        "    }\n",
        "\n",
        "filtered_data = filtered_data.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb9902af-7bbd-4857-9ae1-1893b90abd32",
      "metadata": {},
      "source": [
        "Посмотрим на получившийся пример. В поле `text` должно быть что-то похожее:\n",
        "```\n",
        "<s><|start_header_id|>user<|end_header_id|>\\n\\nкрасивые места ульяновска<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nУльяновск — город ....<|eot_id|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e0392de-c98a-46b4-8dec-1cbf6092d597",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "t9mOKkaytzSc",
        "outputId": "5ed6055b-844f-489a-e6c7-3bacd7c60d0f"
      },
      "outputs": [],
      "source": [
        "filtered_data[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9342d4f0-9f61-4b48-bc3d-4b7389e45613",
      "metadata": {},
      "source": [
        "Теперь зададим data-collator. Этот объект подготавливает тензоры перед форвародом. Обратите внмиание, мы будем использовать `DataCollatorForCompletionOnlyLM`. Он позволяет обучаться только ответах модели, то есть на токенах после `assistant<|end_header_id|>\\n\\n`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1764e61e-359c-4597-ad01-6323df5d6efb",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.add_bos_token = False # при токенизации не добавляем bos, т.к. он есть в чат-шаблоне и уже находится в сэмплах.\n",
        "# Предварительно токенизированная часть  assistant<|end_header_id|>\\n\\n\n",
        "tokenized_assistant_response_template = [108651, 125906, 125853, 617, 125865, 8264, 125865, 367, 73136, 3, 3]\n",
        "# Обучаться будем только на ответах модели, то есть токенах после assistant<|end_header_id|>\\n\\n\n",
        "collator = DataCollatorForCompletionOnlyLM(\n",
        "    # явно указываем начало assistant-реплики, до нее включительно будет стоять -100 для подсчет Cross-Entropy\n",
        "    response_template=tokenized_assistant_response_template,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2af12384-bec0-489d-9054-ddc13a7f8534",
      "metadata": {},
      "source": [
        "Зададим трейнер."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3dd3d84-9a76-4096-82d6-dd25fa809f5c",
      "metadata": {
        "id": "ZZUrZ1yrvVFu"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=...,\n",
        "    tokenizer=...,\n",
        "    train_dataset=...,\n",
        "    # Передаем поле, в котором отформатированные сообщения\n",
        "    # Токенизацию выполнит trainer\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGT,\n",
        "    dataset_num_proc=2,\n",
        "    data_collator=collator,\n",
        "    packing=False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        # На след. занятии поговорим об этом\n",
        "        gradient_accumulation_steps=4,\n",
        "        # Используем небольшой warmup\n",
        "        warmup_steps=5,\n",
        "        # Для знакомства модели с SFT форматом будет достаточно\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        # На след. занятии поговорим об этом\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        # На след. занятии поговорим об этом\n",
        "        optim=\"adamw_8bit\",\n",
        "        # Регуляризация\n",
        "        weight_decay=0.01,\n",
        "        # Линейный шедулер\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b2b4223-99b0-49fa-9e9b-b5eb4a89f5dd",
      "metadata": {
        "id": "Ij1XyZdfeil5"
      },
      "source": [
        "Посмотрим, как работает `DataCollatorForCompletionOnlyLM`. Обратите внимание, что в `labels` имеется некоторый префикс из `-100`. Так мы обучаем модель предсказывать токены реплики модели, игнорируя при подсчете лосса запрос юзера и системный промпт. При этом, в наших данных сейчас только одношаговые диалоги. В более продвинутом случае (многошаговые диалоги) процесс маскирования реплик модели был бы немного сложнее."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b8e57f-bb16-488b-9d48-b5cd2ab8b8f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fP90odLUY3QQ",
        "outputId": "f976c78a-14ee-43dc-91be-88141c846c69"
      },
      "outputs": [],
      "source": [
        "collator([trainer.train_dataset[7][\"input_ids\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a1c72e2-15c3-4453-822c-e1f3d3a68e5c",
      "metadata": {},
      "source": [
        "Обучим модель. Если вы нигде значительно не ошиблись, то лосс будет в районе 0.7 – 1.4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d670bbb1-297c-4ed3-a67c-95d7b0d809b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Qzd1GIamvVsU",
        "outputId": "33fa9c66-9843-43b1-f934-2aa2e9af573c"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71cbc8a3-3d69-4469-8024-104264008e4a",
      "metadata": {
        "id": "vaqbK-CzzSKs"
      },
      "source": [
        "Настало время проверить модель на разных запросах. Сделать это можно так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d220df-ffeb-40dd-86d1-313d8e529f08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "g0bO4BfgxNk-",
        "outputId": "9e9f6ff6-d2e7-4fa7-94e3-8a23f46b0c00"
      },
      "outputs": [],
      "source": [
        "# Перевод модели в eval-model\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Уже знакомый код генерации\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Расскажи коротко, что такое лог регрессия?\"},\n",
        "]\n",
        "\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True, # Использование KV-cache, а не пересчет\n",
        "    do_sample=True,\n",
        "    temperature=0.6\n",
        ")\n",
        "tokenizer.batch_decode(outputs)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7095bc-0a67-486d-8d78-e3b067c5e7c6",
      "metadata": {},
      "source": [
        "Если вы все сделали корректно, то модель точно должны выучить SFT-формат и в целом ответить на запрос, при чем достаточно развернуто."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a577a21-1725-4826-b23e-b614862296f0",
      "metadata": {},
      "source": [
        "Обучите еще один вариант модели, достаточно отличающийся от начального. Например, вы можете использовать другие эвристики для фильтрации датасета или кардинально, но разумно изменить гиперпараметры обучения или LoRA-адаптеров.\n",
        "Сравните две получившиеся модели на 5-10 содержательных запросах, оцените результаты и сделайте выводы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d062b9-445c-4830-a449-4c58ed7e0225",
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}