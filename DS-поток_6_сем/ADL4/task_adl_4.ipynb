{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qyq8YF-6S8s"
   },
   "source": [
    "# DS-поток, весна 2025\n",
    "## Задание ADL.4\n",
    "### Подходы к оптимизации процесса обучения LLMs.\n",
    "\n",
    "**Правила:**\n",
    "\n",
    "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
    "* Выполненную работу нужно отправить телеграм-боту `@miptstats_ds24_bot`. Для начала работы с ботом каждый раз отправляйте `/start`. Дождитесь подтверждения от бота, что он принял файл. Если подтверждения нет, то что-то не так. **Работы, присланные иным способом, не принимаются.**\n",
    "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
    "* Прислать нужно **ноутбук в формате `ipynb`**.\n",
    "* Следите за размером файлов. **Бот не может принимать файлы весом более 20 Мб.** Если файл получается больше, заранее разделите его на несколько.\n",
    "* Выполнять задание необходимо полностью самостоятельно. **При обнаружении списывания все участники списывания получат штраф.**\n",
    "* Решения, размещенные на каких-либо интернет-ресурсах, не принимаются. Кроме того, публикация решения в открытом доступе может быть приравнена к предоставлении возможности списать.\n",
    "* Для выполнения задания используйте этот ноутбук в качестве основы, ничего не удаляя из него. Можно добавлять необходимое количество ячеек.\n",
    "* Комментарии к решению пишите в markdown-ячейках.\n",
    "* Выполнение задания (ход решения, выводы и пр.) должно быть осуществлено на русском языке.\n",
    "* Если код будет не понятен проверяющему, оценка может быть снижена.\n",
    "* Никакой код из данного задания при проверке запускаться не будет. *Если код студента не выполнен, недописан и т.д., то он не оценивается.*\n",
    "* В каждой задаче не забывайте делать **пояснения и выводы**.\n",
    "* **Код из рассказанных на занятиях ноутбуков** можно использовать без ограничений.\n",
    "\n",
    "**Баллы за задание:**\n",
    "\n",
    "* Реализация &mdash; 80 баллов;\n",
    "* Сравнение и анализ &mdash; 70 баллов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size=\"5\">Дорешка:</font></b>\n",
    "\n",
    "* <b><font size=\"4\">Задача 1</font></b>\n",
    "  * <b><font color=\"#00b565\" size=\"3\">Исправил код</font></b>\n",
    "\n",
    "*Примечание*\n",
    "* <b><font color=\"#00b565\">#00b565 &mdash; цвет для исправления замечаний</font></b>\n",
    "* <b><font color=\"#f09000\">#f09000 &mdash; цвет для решений с нуля</font></b>\n",
    "* проверяющий может иначе классифицировать эти два типа дорешек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:10:24.800456Z",
     "iopub.status.busy": "2025-04-25T14:10:24.800173Z",
     "iopub.status.idle": "2025-04-25T14:10:24.804164Z",
     "shell.execute_reply": "2025-04-25T14:10:24.803417Z",
     "shell.execute_reply.started": "2025-04-25T14:10:24.800436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Bot check\n",
    "\n",
    "# HW_ID: ds_adl4\n",
    "# Бот проверит этот ID и предупредит, если случайно сдать что-то не то.\n",
    "\n",
    "# Status: final\n",
    "# Перед отправкой в финальном решении удали \"not\" в строчке выше.\n",
    "# Так бот проверит, что ты отправляешь финальную версию, а не промежуточную.\n",
    "# Никакие значения в этой ячейке не влияют на факт сдачи работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:10:24.813593Z",
     "iopub.status.busy": "2025-04-25T14:10:24.813391Z",
     "iopub.status.idle": "2025-04-25T14:10:24.823075Z",
     "shell.execute_reply": "2025-04-25T14:10:24.822531Z",
     "shell.execute_reply.started": "2025-04-25T14:10:24.813577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU9xjOMyYurQ"
   },
   "source": [
    "## Описание\n",
    "\n",
    "На занятии мы познакомились с различными техниками, которые используются при обучении больших языковых моделей. В этом домашнем задании вам предстоит решить реальную практическую задачу, которая связана с оптимизацией некоторых слоев в трансформере.\n",
    "\n",
    "Вспомним общую идею техники, которая называется Gradient Checkpointing. Идея заключается в том, чтобы на этапе forward'a не запоминать промежуточные активации, необходимые для backward'a, а вычислять их непосредственно на этапе backward'a. Почему это может быть важно? Оказывается, что активации MLP для больших моделей занимают очень много памяти. Сохранить для backward'а все активации, включая слои внимания, просто невозможно. Возникает вопрос: стоит ли сохранять промежуточные активации MLP или же отдать память под активации attention'a? На практике пересчет активаций для MLP оказывается гораздо быстрее, чем пересчет того же attention'a. В итоге мы можем не сохранять активации MLP, экономить достаточно много памяти, а часть освободившейся памяти отдать под активации attention'a и тем самым даже ускорить обучение!\n",
    "\n",
    "## Реализация\n",
    "Сегодня мы не будем работать с полноценным трансформером, а сфокусируемся только на MLP-блоке. Вам предлагается написать модифицированный слой MLP таким образом, чтобы он поддерживал возможность либо сохранять промежуточные активации, либо пересчитывать их на этапе backward'a. Оформить код нужно будет в виде кастомной `torch.autograd.Function`. Хорошая практика заключается в том, чтобы ваш итоговый слой, который наследуется от `torch.nn.Module` просто вызывал функцию с нужными параметрами. Вам нужно **обязательно** ознакомиться с [постом](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html), в котором вы узнаете, как правильно написать кастомную `torch.autograd.Function` функцию и что это вообще такое.\n",
    "\n",
    "\n",
    "Вспомним с лекции, как выглядит модифицированный MLP для современных арихетктур.\n",
    "$$\n",
    "\\text{FFN}_{\\text{SwiGLU}}\\left(x, W, V, U\\right) = \\left(\\text{Swish}_1\\left(xW\\right)\\otimes xV\\right)U = \\left(\\text{SiLU}\\left(xW\\right)\\otimes xV\\right)U,\n",
    "$$\n",
    "где\n",
    "$$\n",
    "\\text{Swish}_1\\left(x\\right)= \\text{SiLU}\\left(x\\right) = x\\sigma\\left(x\\right)\n",
    "$$\n",
    "\n",
    "Такой MLP-блок используется в [LLaMA](https://arxiv.org/abs/2307.09288)-подобных архитектурах. В этом задании будем использовать его. Посмотрим, как выглядит реализация в виде простого `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"#00b565\" size=\"5\">Исправил код</font></b>\n",
    "\n",
    "<b><font color=\"#00b565\" size=\"3\">Замечание:</font></b> <i>  \n",
    "Реализация.  \n",
    "▫️ Наличие решения. (80.0)  \n",
    "▫️ Неправильно реализован подсчет градиента для матрицы U. Инд. комм. строка silu_xW = torch.sigmoid(xW)*(xV) должна быть c xW, а не xV (-20.0)  \n",
    "▫️ Неправильно реализован подсчет градиента для матрицы W. Инд. комм. тут тоже прокиывается ошибка из-за silu_xw  \n",
    "▫️ Неправильно реализован подсчет градиента для матрицы V. Инд. комм. тут тоже прокиывается ошибка из-за silu_xw  \n",
    "Баллы: 60.0 / 80.0</i>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:10:24.828221Z",
     "iopub.status.busy": "2025-04-25T14:10:24.828033Z",
     "iopub.status.idle": "2025-04-25T14:10:24.839393Z",
     "shell.execute_reply": "2025-04-25T14:10:24.838886Z",
     "shell.execute_reply.started": "2025-04-25T14:10:24.828207Z"
    },
    "id": "5fdC5ZZ4tIsg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SwigluMLP(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.V = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
    "        self.U = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "    def forward(self, x):\n",
    "        output = self.U(self.act_fn(self.W(x)) * self.V(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRH0ZKgcxizx"
   },
   "source": [
    "Теперь перейдем к нашей реализации. Обратите внимание, что `MemoryOptimizedSwigluMLPFunction` принимает на вход `checkpoint_level`. Это переменная нужна для реализации следующей логики:\n",
    "* `checkpoint_level == 0` &mdash; никаких оптимизаций не проводится, промежуточные активации просто сохраняются для переиспользования на этапе backward'a через `ctx.save_for_backward(...)`;\n",
    "* `checkpoint_level == 1` &mdash; для backward'a сохраняются только вход `x` и матрицы `W, V, U`, а на этапе backward'a нужные активации просто снова пересчитываются.\n",
    "\n",
    "Вам нужно реализовать методы `forward` и `backward`. Реализация второго потребует от вас посчитать некоторые промежуточные градиенты в матричном виде. Обязательно **выпишите и поясните** получающиеся формулы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{d}{dx}SiLU(x) = x' \\sigma(x) + x \\sigma'(x) = \\sigma(x) + x\\sigma(x)(1 - \\sigma(x))$  \n",
    "$\\frac{d}{dW}out = \\frac{d}{dW} ((SiLU(xW) \\otimes xV)U) = ( \\frac{d SiLU(xW)}{d W} \\otimes xV ) U = ( x ( \\sigma(xW) + xW \\sigma(xW) (1 - \\sigma(xW)) ) \\otimes xV ) U$  \n",
    "$\\frac{d}{dV}out = \\frac{d}{dV} ((SiLU(xW) \\otimes xV)U) = (SiLU(xW) \\otimes \\frac{d (xV)}{dV}) U = (SiLU(xW) \\otimes x) U$  \n",
    "$\\frac{d}{dU}out = \\frac{d}{dU} ((SiLU(xW) \\otimes xV)U) = (SiLU(xW) \\otimes xV)$  \n",
    "*Пояснение: chain rule. Надеюсь, расписал без опечаток)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:10:24.843701Z",
     "iopub.status.busy": "2025-04-25T14:10:24.843474Z",
     "iopub.status.idle": "2025-04-25T14:10:24.861332Z",
     "shell.execute_reply": "2025-04-25T14:10:24.860824Z",
     "shell.execute_reply.started": "2025-04-25T14:10:24.843685Z"
    },
    "id": "rPK-tVC_xmgW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MemoryOptimizedSwigluMLPFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, W, V, U, checkpoint_level):  # x размера (batch_size,seq_len,hidden_dim)\n",
    "        xW,xV = torch.matmul(x,W), torch.matmul(x,V)\n",
    "        silu_xW = torch.sigmoid(xW)*xW\n",
    "        if checkpoint_level == 0:\n",
    "            ctx.save_for_backward(x,W,V,U,xW,xV,silu_xW)\n",
    "        else:\n",
    "            ctx.save_for_backward(x,W,V,U)\n",
    "        ctx.checkpoint_level = checkpoint_level\n",
    "        return torch.matmul((silu_xW*xV),U)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x,W,V,U,*etc = ctx.saved_tensors\n",
    "        if ctx.checkpoint_level == 0:\n",
    "            xW,xV,silu_xW = etc\n",
    "        else:\n",
    "            xW,xV = torch.matmul(x,W), torch.matmul(x,V)\n",
    "            silu_xW = torch.sigmoid(xW)*(xW)  # фикс\n",
    "        grad_xW,grad_xV = torch.matmul(grad_output,U.t())*(torch.sigmoid(xW)+xW*torch.sigmoid(xW)*(1-torch.sigmoid(xW))), torch.matmul(grad_output,U.t())*silu_xW\n",
    "        grad_W,grad_V,grad_U = x.transpose(1,2)@grad_xW, x.transpose(1,2)@grad_xV, (silu_xW*xV).transpose(1,2)@grad_output\n",
    "        grad_x = torch.matmul(grad_xW,W.t())+torch.matmul(grad_xV,V.t())\n",
    "        return grad_x,grad_W,grad_V,grad_U,None\n",
    "\n",
    "class MemoryOptimizedSwigluMLP(nn.Module):  # оптимизированный MLP-слой\n",
    "    def __init__(self, hidden_size, intermediate_size, checkpoint_level=0):\n",
    "        super(MemoryOptimizedSwigluMLP, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(intermediate_size, hidden_size))\n",
    "        self.V = nn.Parameter(torch.randn(intermediate_size, hidden_size))\n",
    "        self.U = nn.Parameter(torch.randn(hidden_size, intermediate_size))\n",
    "        self.checkpoint_level = checkpoint_level\n",
    "    def forward(self, x):\n",
    "        return MemoryOptimizedSwigluMLPFunction.apply(x,self.W,self.V,self.U,self.checkpoint_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZaKpdWgt0lqR"
   },
   "source": [
    "Теперь проверим, что реализованный MLP-слой считается верно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:10:24.863897Z",
     "iopub.status.busy": "2025-04-25T14:10:24.863697Z",
     "iopub.status.idle": "2025-04-25T14:10:25.347967Z",
     "shell.execute_reply": "2025-04-25T14:10:25.347164Z",
     "shell.execute_reply.started": "2025-04-25T14:10:24.863882Z"
    },
    "id": "-PaLwg0_0qlZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size, seq_len, hidden_dim = 4, 256, 768  # Зададим параметры\n",
    "\n",
    "dummy_input = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "# Обратите внимание, что intermediate_size кратно больше hidden_dim\n",
    "# Это типичное \"расширение\", характерное для MLP (FFN) слоя в трансформере\n",
    "swiglu_mlp = SwigluMLP(hidden_dim, hidden_dim * 3)\n",
    "optimized_swiglu_mlp = MemoryOptimizedSwigluMLP(hidden_dim, hidden_dim * 3)\n",
    "\n",
    "with torch.no_grad():  # Скопируем параметры, чтобы они были одинаковые\n",
    "    optimized_swiglu_mlp.W.data = swiglu_mlp.W.weight.data.t()\n",
    "    optimized_swiglu_mlp.V.data = swiglu_mlp.V.weight.data.t()\n",
    "    optimized_swiglu_mlp.U.data = swiglu_mlp.U.weight.data.t()\n",
    "\n",
    "standard_output = swiglu_mlp(dummy_input)  # Прогоним модель\n",
    "optimized_output = optimized_swiglu_mlp(dummy_input)\n",
    "assert torch.allclose(standard_output, optimized_output, atol=1e-4)  # Проверим выходы слоев на совпадение\n",
    "\n",
    "standard_output.sum().backward()\n",
    "optimized_output.sum().backward()\n",
    "assert torch.allclose(swiglu_mlp.U.weight.grad, optimized_swiglu_mlp.U.grad.t(), atol=1e-4)  # Проверим на совпадение градиенты\n",
    "assert torch.allclose(swiglu_mlp.V.weight.grad, optimized_swiglu_mlp.V.grad.t(), atol=1e-4)\n",
    "assert torch.allclose(swiglu_mlp.W.weight.grad, optimized_swiglu_mlp.W.grad.t(), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Все работает*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font color=\"#00b565\" size=\"5\">===== Конец исправлений =====</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytfh5Nzk4F7p"
   },
   "source": [
    "## Сравнение и анализ\n",
    "\n",
    "Сравните время исполнение forward/backward и объем потребляемой памяти в зависимости от значения `checkpoint_level`. Проведите эксперименты для разных значений `batch_size`, `seq_len`, `hidden_dim`. Сделайте запуски в нескольких сетапах. Попробуйте достаточно большие `seq_len=1024` и `hidden_dim=4096`, а также `num_layers=5`. Сделайте выводы.\n",
    "\n",
    "Некоторые советы:\n",
    "* Для отслеживания потребляемой памяти можете воспользоваться `torch.cuda.max_memory_allocated()`. Желательно после каждого шага очищать статистику, используя `torch.cuda.reset_peak_memory_stats()`. Более подробно рекомендуется почитать документацию по [ссылке](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management).\n",
    "* Для подсчета времени можно воспользоваться простым ` time.perf_counter()` или же `time.time()`. Однако с подсчетом времени для GPU-операций все немного хитрее. В PyTorch и других библиотеках, работающих с GPU, операции выполняются асинхронно по отношению к коду, исполняемому на CPU. Такой подход позволяет CPU продолжать работу, не ожидая окончания каждой операции на GPU, что способствует повышению общей производительности за счёт параллельной работы CPU и GPU. Что это значит на практике? Вы можете получить завышенные результаты своих измерений, так как замер времени может завершиться до того, как GPU в действительности закончит выполнение операций.\n",
    "Рассмотрим пример кода:\n",
    "```\n",
    "a = torch.rand(10000, 10000, device=\"cuda\")\n",
    "start_time = time.time()\n",
    "b = a @ a\n",
    "elapsed_time = time.time() - start_time\n",
    "```\n",
    "В этом примере, после запуска операции умножения матриц `a @ a`, мы немедленно измеряем время выполнения функции, не дожидаясь её фактического завершения на GPU.\n",
    "Для получения точных измерений времени выполнения операций на GPU необходимо использовать синхронизацию. В PyTorch это можно сделать с помощью функции `torch.cuda.synchronize()`, которая блокирует выполнение кода на CPU до тех пор, пока все запланированные задачи на соответствующем GPU не будут завершены. \\\n",
    "Пример более грамотного кода:\n",
    "```\n",
    "a = torch.rand(10000, 10000, device='cuda')\n",
    "torch.cuda.synchronize() # ждем завершения всех предыдущих операций на GPU\n",
    "start_time = time.time()\n",
    "b = a @ a\n",
    "torch.cuda.synchronize() # cнова синхронизируемся, чтобы убедиться, что операция завершена\n",
    "elapsed_time = time.time() - start_time\n",
    "```\n",
    "* Для подсчета статистики следует сделать несколько проходов forward/backward для одной модели, а полученные результаты просто усреднить. Для более стабильных результатов выполните также разогрев, то есть некоторое количество прогонов модели перед основным измерением. Это важно, т.к. на результаты измерений могут повлиять дополнительные задержки, связанные с инициализацией и загрузкой ресурсов, температурой GPU, а также различные кэши.\n",
    "* Представьте результаты и выводы в информативном виде, хорошо подойдет какая-нибудь табличка. Затраты по памяти лучше всего указать в Гб, а время исполнения в секундах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:10:25.359277Z",
     "iopub.status.busy": "2025-04-25T14:10:25.359089Z",
     "iopub.status.idle": "2025-04-25T14:10:25.386692Z",
     "shell.execute_reply": "2025-04-25T14:10:25.385937Z",
     "shell.execute_reply.started": "2025-04-25T14:10:25.359261Z"
    },
    "id": "nreczZ67-cKo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# используем несколько слоев, чтобы увидеть выигрыш по памяти\n",
    "# в случае chechkpoint_level == 0 для вычислений очередного слоя будет использована та память, что осталась для предыдущего\n",
    "# если же checkpoint_level == 1, то придется хранить активации для всех слоев\n",
    "\n",
    "class TestTransformer(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, intermediate_size, checkpoint_level):\n",
    "        super(TestTransformer, self).__init__()\n",
    "        self.layers = nn.ModuleList([MemoryOptimizedSwigluMLP(hidden_size, intermediate_size, checkpoint_level) for _ in range(num_layers)])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация функции для бенчмарка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:10:25.388424Z",
     "iopub.status.busy": "2025-04-25T14:10:25.388135Z",
     "iopub.status.idle": "2025-04-25T14:10:25.409118Z",
     "shell.execute_reply": "2025-04-25T14:10:25.408413Z",
     "shell.execute_reply.started": "2025-04-25T14:10:25.388397Z"
    },
    "id": "m3qiIeNw-4xE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def benchmark_transformer(model, batch_size, seq_len, hidden_dim, num_warmup_steps, num_steps):\n",
    "    statistics = defaultdict(list)\n",
    "    input_tensor = torch.randn(batch_size, seq_len, hidden_dim, device='cuda')  # Генерация входных данных\n",
    "    for _ in range(num_warmup_steps):  # Warmup GPU чтобы избежать заниженного перфоманса\n",
    "        model(input_tensor)\n",
    "        torch.cuda.synchronize()\n",
    "    for _ in range(num_steps):  # Основное измерение\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start = time.time()\n",
    "        output = model(input_tensor)\n",
    "        torch.cuda.synchronize()\n",
    "        statistics['time_fwd'].append(time.time()-start)\n",
    "        start = time.time()\n",
    "        output.sum().backward()\n",
    "        torch.cuda.synchronize()\n",
    "        statistics['time_bwd'].append(time.time()-start)\n",
    "        statistics['memory'].append(torch.cuda.max_memory_allocated() / (1024**3))  # в гб\n",
    "    for k,v in statistics.items():\n",
    "        statistics[k] = sum(v) / num_steps\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T14:10:25.409953Z",
     "iopub.status.busy": "2025-04-25T14:10:25.409773Z",
     "iopub.status.idle": "2025-04-25T15:38:26.649921Z",
     "shell.execute_reply": "2025-04-25T15:38:26.648479Z",
     "shell.execute_reply.started": "2025-04-25T14:10:25.409938Z"
    },
    "id": "1uZmZUcCwt8j",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 256 1024 3 0\n",
      "2 256 1024 3 1\n",
      "2 256 1024 4 0\n",
      "2 256 1024 4 1\n",
      "2 256 1024 5 0\n",
      "2 256 1024 5 1\n",
      "2 256 2048 3 0\n",
      "2 256 2048 3 1\n",
      "2 256 2048 4 0\n",
      "2 256 2048 4 1\n",
      "2 256 2048 5 0\n",
      "2 256 2048 5 1\n",
      "2 256 4096 3 0\n",
      "2 256 4096 3 1\n",
      "2 256 4096 4 0\n",
      "2 256 4096 4 1\n",
      "2 256 4096 5 0\n",
      "2 256 4096 5 1\n",
      "2 512 1024 3 0\n",
      "2 512 1024 3 1\n",
      "2 512 1024 4 0\n",
      "2 512 1024 4 1\n",
      "2 512 1024 5 0\n",
      "2 512 1024 5 1\n",
      "2 512 2048 3 0\n",
      "2 512 2048 3 1\n",
      "2 512 2048 4 0\n",
      "2 512 2048 4 1\n",
      "2 512 2048 5 0\n",
      "2 512 2048 5 1\n",
      "2 512 4096 3 0\n",
      "2 512 4096 3 1\n",
      "2 512 4096 4 0\n",
      "2 512 4096 4 1\n",
      "2 512 4096 5 0\n",
      "2 512 4096 5 1\n",
      "2 1024 1024 3 0\n",
      "2 1024 1024 3 1\n",
      "2 1024 1024 4 0\n",
      "2 1024 1024 4 1\n",
      "2 1024 1024 5 0\n",
      "2 1024 1024 5 1\n",
      "2 1024 2048 3 0\n",
      "2 1024 2048 3 1\n",
      "2 1024 2048 4 0\n",
      "2 1024 2048 4 1\n",
      "2 1024 2048 5 0\n",
      "2 1024 2048 5 1\n",
      "2 1024 4096 3 0\n",
      "2 1024 4096 3 1\n",
      "2 1024 4096 4 0\n",
      "2 1024 4096 4 1\n",
      "2 1024 4096 5 0\n",
      "2 1024 4096 5 1\n",
      "4 256 1024 3 0\n",
      "4 256 1024 3 1\n",
      "4 256 1024 4 0\n",
      "4 256 1024 4 1\n",
      "4 256 1024 5 0\n",
      "4 256 1024 5 1\n",
      "4 256 2048 3 0\n",
      "4 256 2048 3 1\n",
      "4 256 2048 4 0\n",
      "4 256 2048 4 1\n",
      "4 256 2048 5 0\n",
      "4 256 2048 5 1\n",
      "4 256 4096 3 0\n",
      "4 256 4096 3 1\n",
      "4 256 4096 4 0\n",
      "4 256 4096 4 1\n",
      "4 256 4096 5 0\n",
      "4 256 4096 5 1\n",
      "4 512 1024 3 0\n",
      "4 512 1024 3 1\n",
      "4 512 1024 4 0\n",
      "4 512 1024 4 1\n",
      "4 512 1024 5 0\n",
      "4 512 1024 5 1\n",
      "4 512 2048 3 0\n",
      "4 512 2048 3 1\n",
      "4 512 2048 4 0\n",
      "4 512 2048 4 1\n",
      "4 512 2048 5 0\n",
      "4 512 2048 5 1\n",
      "4 512 4096 3 0\n",
      "4 512 4096 3 1\n",
      "4 512 4096 4 0\n",
      "4 512 4096 4 1\n",
      "4 512 4096 5 0\n",
      "4 512 4096 5 1\n",
      "4 1024 1024 3 0\n",
      "4 1024 1024 3 1\n",
      "4 1024 1024 4 0\n",
      "4 1024 1024 4 1\n",
      "4 1024 1024 5 0\n",
      "4 1024 1024 5 1\n",
      "4 1024 2048 3 0\n",
      "4 1024 2048 3 1\n",
      "4 1024 2048 4 0\n",
      "4 1024 2048 4 1\n",
      "4 1024 2048 5 0\n",
      "4 1024 2048 5 1\n",
      "4 1024 4096 3 0\n",
      "4 1024 4096 3 1\n",
      "4 1024 4096 4 0\n",
      "4 1024 4096 4 1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 240.12 MiB is free. Process 6615 has 14.50 GiB memory in use. Of the allocated memory 13.62 GiB is allocated by PyTorch, and 771.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/367320309.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbenchmark_transformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                         \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'seq_len'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hidden_dim'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhid_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_layers'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_ls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'checkpoint_level'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlvl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/4189488965.py\u001b[0m in \u001b[0;36mbenchmark_transformer\u001b[0;34m(model, batch_size, seq_len, hidden_dim, num_warmup_steps, num_steps)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstatistics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_fwd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstatistics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_bwd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    305\u001b[0m             )\n\u001b[1;32m    306\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/1348909379.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0msilu_xW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgrad_xW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_xV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mxW\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msilu_xW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mgrad_W\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_V\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_U\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mgrad_xW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mgrad_xV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msilu_xW\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mgrad_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mgrad_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_xW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_xV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_V\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_U\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 240.12 MiB is free. Process 6615 has 14.50 GiB memory in use. Of the allocated memory 13.62 GiB is allocated by PyTorch, and 771.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, hidden_dim, num_layers = 4, 1024, 4096, 5\n",
    "stats = {'batch_size':[],'seq_len':[],'hidden_dim':[],'num_layers':[],'checkpoint_level':[],'time_fwd':[],'memory':[],'time_bwd':[]}\n",
    "for bs in [2,4,8]:\n",
    "    for seq_len in [256,512,1024]:\n",
    "        for hid_dim in [1024,2048,4096]:\n",
    "            for num_ls in [3,4,5]:\n",
    "                for lvl in [0,1]:\n",
    "                    model = TestTransformer(num_ls, hid_dim, hid_dim*3, lvl)\n",
    "                    model.cuda()\n",
    "                    for k,v in benchmark_transformer(model, bs, seq_len, hid_dim, 10, 50).items():\n",
    "                        stats[k].append(v)\n",
    "                    for k,v in {'batch_size':bs,'seq_len':seq_len,'hidden_dim':hid_dim,'num_layers':num_ls,'checkpoint_level':lvl}.items():\n",
    "                        stats[k].append(v)\n",
    "                    print(bs, seq_len, hid_dim, num_ls, lvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Памяти перестало хватать, посмотрим на то, что посчиталось*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T15:48:58.954662Z",
     "iopub.status.busy": "2025-04-25T15:48:58.954115Z",
     "iopub.status.idle": "2025-04-25T15:48:59.022423Z",
     "shell.execute_reply": "2025-04-25T15:48:59.021906Z",
     "shell.execute_reply.started": "2025-04-25T15:48:58.954640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009</td>\n",
       "      <td>3.104</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009</td>\n",
       "      <td>3.070</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011</td>\n",
       "      <td>3.194</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011</td>\n",
       "      <td>3.142</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013</td>\n",
       "      <td>3.283</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014</td>\n",
       "      <td>3.214</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037</td>\n",
       "      <td>4.025</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037</td>\n",
       "      <td>3.956</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041</td>\n",
       "      <td>4.345</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.042</td>\n",
       "      <td>4.241</td>\n",
       "      <td>0.141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "0           2      256        1024           3                 0     0.009   \n",
       "1           2      256        1024           3                 1     0.009   \n",
       "2           2      256        1024           4                 0     0.011   \n",
       "3           2      256        1024           4                 1     0.011   \n",
       "4           2      256        1024           5                 0     0.013   \n",
       "5           2      256        1024           5                 1     0.014   \n",
       "6           2      256        2048           3                 0     0.037   \n",
       "7           2      256        2048           3                 1     0.037   \n",
       "8           2      256        2048           4                 0     0.041   \n",
       "9           2      256        2048           4                 1     0.042   \n",
       "\n",
       "   memory  time_bwd  \n",
       "0   3.104     0.024  \n",
       "1   3.070     0.030  \n",
       "2   3.194     0.029  \n",
       "3   3.142     0.037  \n",
       "4   3.283     0.035  \n",
       "5   3.214     0.045  \n",
       "6   4.025     0.094  \n",
       "7   3.956     0.117  \n",
       "8   4.345     0.112  \n",
       "9   4.241     0.141  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.053</td>\n",
       "      <td>4.664</td>\n",
       "      <td>0.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.053</td>\n",
       "      <td>4.526</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138</td>\n",
       "      <td>7.551</td>\n",
       "      <td>0.382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.137</td>\n",
       "      <td>7.413</td>\n",
       "      <td>0.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174</td>\n",
       "      <td>8.753</td>\n",
       "      <td>0.420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.175</td>\n",
       "      <td>8.546</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.214</td>\n",
       "      <td>9.954</td>\n",
       "      <td>0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.213</td>\n",
       "      <td>9.679</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018</td>\n",
       "      <td>3.184</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018</td>\n",
       "      <td>3.115</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "10           2      256        2048           5                 0     0.053   \n",
       "11           2      256        2048           5                 1     0.053   \n",
       "12           2      256        4096           3                 0     0.138   \n",
       "13           2      256        4096           3                 1     0.137   \n",
       "14           2      256        4096           4                 0     0.174   \n",
       "15           2      256        4096           4                 1     0.175   \n",
       "16           2      256        4096           5                 0     0.214   \n",
       "17           2      256        4096           5                 1     0.213   \n",
       "18           2      512        1024           3                 0     0.018   \n",
       "19           2      512        1024           3                 1     0.018   \n",
       "\n",
       "    memory  time_bwd  \n",
       "10   4.664     0.132  \n",
       "11   4.526     0.166  \n",
       "12   7.551     0.382  \n",
       "13   7.413     0.466  \n",
       "14   8.753     0.420  \n",
       "15   8.546     0.535  \n",
       "16   9.954     0.520  \n",
       "17   9.679     0.656  \n",
       "18   3.184     0.043  \n",
       "19   3.115     0.055  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>3.292</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021</td>\n",
       "      <td>3.189</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026</td>\n",
       "      <td>3.401</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026</td>\n",
       "      <td>3.263</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.071</td>\n",
       "      <td>4.184</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.072</td>\n",
       "      <td>4.046</td>\n",
       "      <td>0.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086</td>\n",
       "      <td>4.542</td>\n",
       "      <td>0.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086</td>\n",
       "      <td>4.335</td>\n",
       "      <td>0.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.107</td>\n",
       "      <td>4.899</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106</td>\n",
       "      <td>4.624</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "20           2      512        1024           4                 0     0.021   \n",
       "21           2      512        1024           4                 1     0.021   \n",
       "22           2      512        1024           5                 0     0.026   \n",
       "23           2      512        1024           5                 1     0.026   \n",
       "24           2      512        2048           3                 0     0.071   \n",
       "25           2      512        2048           3                 1     0.072   \n",
       "26           2      512        2048           4                 0     0.086   \n",
       "27           2      512        2048           4                 1     0.086   \n",
       "28           2      512        2048           5                 0     0.107   \n",
       "29           2      512        2048           5                 1     0.106   \n",
       "\n",
       "    memory  time_bwd  \n",
       "20   3.292     0.054  \n",
       "21   3.189     0.069  \n",
       "22   3.401     0.065  \n",
       "23   3.263     0.083  \n",
       "24   4.184     0.175  \n",
       "25   4.046     0.220  \n",
       "26   4.542     0.206  \n",
       "27   4.335     0.262  \n",
       "28   4.899     0.244  \n",
       "29   4.624     0.313  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.303</td>\n",
       "      <td>7.868</td>\n",
       "      <td>0.732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.300</td>\n",
       "      <td>7.592</td>\n",
       "      <td>0.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.387</td>\n",
       "      <td>9.146</td>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.386</td>\n",
       "      <td>8.732</td>\n",
       "      <td>1.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468</td>\n",
       "      <td>10.424</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465</td>\n",
       "      <td>9.873</td>\n",
       "      <td>1.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035</td>\n",
       "      <td>3.344</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036</td>\n",
       "      <td>3.204</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043</td>\n",
       "      <td>3.492</td>\n",
       "      <td>0.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043</td>\n",
       "      <td>3.282</td>\n",
       "      <td>0.136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "30           2      512        4096           3                 0     0.303   \n",
       "31           2      512        4096           3                 1     0.300   \n",
       "32           2      512        4096           4                 0     0.387   \n",
       "33           2      512        4096           4                 1     0.386   \n",
       "34           2      512        4096           5                 0     0.468   \n",
       "35           2      512        4096           5                 1     0.465   \n",
       "36           2     1024        1024           3                 0     0.035   \n",
       "37           2     1024        1024           3                 1     0.036   \n",
       "38           2     1024        1024           4                 0     0.043   \n",
       "39           2     1024        1024           4                 1     0.043   \n",
       "\n",
       "    memory  time_bwd  \n",
       "30   7.868     0.732  \n",
       "31   7.592     0.932  \n",
       "32   9.146     0.806  \n",
       "33   8.732     1.072  \n",
       "34  10.424     0.997  \n",
       "35   9.873     1.319  \n",
       "36   3.344     0.084  \n",
       "37   3.204     0.107  \n",
       "38   3.492     0.106  \n",
       "39   3.282     0.136  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055</td>\n",
       "      <td>3.639</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055</td>\n",
       "      <td>3.360</td>\n",
       "      <td>0.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154</td>\n",
       "      <td>4.502</td>\n",
       "      <td>0.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.153</td>\n",
       "      <td>4.225</td>\n",
       "      <td>0.451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.200</td>\n",
       "      <td>4.936</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.197</td>\n",
       "      <td>4.522</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245</td>\n",
       "      <td>5.371</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.244</td>\n",
       "      <td>4.818</td>\n",
       "      <td>0.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.521</td>\n",
       "      <td>8.502</td>\n",
       "      <td>1.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521</td>\n",
       "      <td>7.951</td>\n",
       "      <td>1.713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "40           2     1024        1024           5                 0     0.055   \n",
       "41           2     1024        1024           5                 1     0.055   \n",
       "42           2     1024        2048           3                 0     0.154   \n",
       "43           2     1024        2048           3                 1     0.153   \n",
       "44           2     1024        2048           4                 0     0.200   \n",
       "45           2     1024        2048           4                 1     0.197   \n",
       "46           2     1024        2048           5                 0     0.245   \n",
       "47           2     1024        2048           5                 1     0.244   \n",
       "48           2     1024        4096           3                 0     0.521   \n",
       "49           2     1024        4096           3                 1     0.521   \n",
       "\n",
       "    memory  time_bwd  \n",
       "40   3.639     0.125  \n",
       "41   3.360     0.159  \n",
       "42   4.502     0.346  \n",
       "43   4.225     0.451  \n",
       "44   4.936     0.399  \n",
       "45   4.522     0.536  \n",
       "46   5.371     0.478  \n",
       "47   4.818     0.646  \n",
       "48   8.502     1.380  \n",
       "49   7.951     1.713  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.668</td>\n",
       "      <td>9.933</td>\n",
       "      <td>1.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.669</td>\n",
       "      <td>9.106</td>\n",
       "      <td>1.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818</td>\n",
       "      <td>11.364</td>\n",
       "      <td>1.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816</td>\n",
       "      <td>10.262</td>\n",
       "      <td>2.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.018</td>\n",
       "      <td>3.254</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018</td>\n",
       "      <td>3.185</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>3.363</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021</td>\n",
       "      <td>3.259</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026</td>\n",
       "      <td>3.471</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026</td>\n",
       "      <td>3.333</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "50           2     1024        4096           4                 0     0.668   \n",
       "51           2     1024        4096           4                 1     0.669   \n",
       "52           2     1024        4096           5                 0     0.818   \n",
       "53           2     1024        4096           5                 1     0.816   \n",
       "54           4      256        1024           3                 0     0.018   \n",
       "55           4      256        1024           3                 1     0.018   \n",
       "56           4      256        1024           4                 0     0.021   \n",
       "57           4      256        1024           4                 1     0.021   \n",
       "58           4      256        1024           5                 0     0.026   \n",
       "59           4      256        1024           5                 1     0.026   \n",
       "\n",
       "    memory  time_bwd  \n",
       "50   9.933     1.536  \n",
       "51   9.106     1.979  \n",
       "52  11.364     1.910  \n",
       "53  10.262     2.444  \n",
       "54   3.254     0.044  \n",
       "55   3.185     0.056  \n",
       "56   3.363     0.054  \n",
       "57   3.259     0.069  \n",
       "58   3.471     0.065  \n",
       "59   3.333     0.083  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.070</td>\n",
       "      <td>4.465</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071</td>\n",
       "      <td>4.327</td>\n",
       "      <td>0.221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086</td>\n",
       "      <td>4.823</td>\n",
       "      <td>0.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.086</td>\n",
       "      <td>4.616</td>\n",
       "      <td>0.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.106</td>\n",
       "      <td>5.181</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.107</td>\n",
       "      <td>4.905</td>\n",
       "      <td>0.315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301</td>\n",
       "      <td>8.993</td>\n",
       "      <td>0.746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.301</td>\n",
       "      <td>8.717</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.388</td>\n",
       "      <td>10.271</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.385</td>\n",
       "      <td>9.857</td>\n",
       "      <td>1.087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "60           4      256        2048           3                 0     0.070   \n",
       "61           4      256        2048           3                 1     0.071   \n",
       "62           4      256        2048           4                 0     0.086   \n",
       "63           4      256        2048           4                 1     0.086   \n",
       "64           4      256        2048           5                 0     0.106   \n",
       "65           4      256        2048           5                 1     0.107   \n",
       "66           4      256        4096           3                 0     0.301   \n",
       "67           4      256        4096           3                 1     0.301   \n",
       "68           4      256        4096           4                 0     0.388   \n",
       "69           4      256        4096           4                 1     0.385   \n",
       "\n",
       "    memory  time_bwd  \n",
       "60   4.465     0.176  \n",
       "61   4.327     0.221  \n",
       "62   4.823     0.208  \n",
       "63   4.616     0.262  \n",
       "64   5.181     0.245  \n",
       "65   4.905     0.315  \n",
       "66   8.993     0.746  \n",
       "67   8.717     0.950  \n",
       "68  10.271     0.822  \n",
       "69   9.857     1.087  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468</td>\n",
       "      <td>11.549</td>\n",
       "      <td>1.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.466</td>\n",
       "      <td>10.998</td>\n",
       "      <td>1.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036</td>\n",
       "      <td>3.414</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.036</td>\n",
       "      <td>3.275</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043</td>\n",
       "      <td>3.562</td>\n",
       "      <td>0.107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044</td>\n",
       "      <td>3.353</td>\n",
       "      <td>0.137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056</td>\n",
       "      <td>3.710</td>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055</td>\n",
       "      <td>3.431</td>\n",
       "      <td>0.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.154</td>\n",
       "      <td>4.783</td>\n",
       "      <td>0.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154</td>\n",
       "      <td>4.507</td>\n",
       "      <td>0.455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "70           4      256        4096           5                 0     0.468   \n",
       "71           4      256        4096           5                 1     0.466   \n",
       "72           4      512        1024           3                 0     0.036   \n",
       "73           4      512        1024           3                 1     0.036   \n",
       "74           4      512        1024           4                 0     0.043   \n",
       "75           4      512        1024           4                 1     0.044   \n",
       "76           4      512        1024           5                 0     0.056   \n",
       "77           4      512        1024           5                 1     0.055   \n",
       "78           4      512        2048           3                 0     0.154   \n",
       "79           4      512        2048           3                 1     0.154   \n",
       "\n",
       "    memory  time_bwd  \n",
       "70  11.549     1.012  \n",
       "71  10.998     1.338  \n",
       "72   3.414     0.084  \n",
       "73   3.275     0.107  \n",
       "74   3.562     0.107  \n",
       "75   3.353     0.137  \n",
       "76   3.710     0.124  \n",
       "77   3.431     0.159  \n",
       "78   4.783     0.351  \n",
       "79   4.507     0.455  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198</td>\n",
       "      <td>5.218</td>\n",
       "      <td>0.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.199</td>\n",
       "      <td>4.803</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244</td>\n",
       "      <td>5.652</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.244</td>\n",
       "      <td>5.100</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.520</td>\n",
       "      <td>9.627</td>\n",
       "      <td>1.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.520</td>\n",
       "      <td>9.076</td>\n",
       "      <td>1.734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.669</td>\n",
       "      <td>11.058</td>\n",
       "      <td>1.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.670</td>\n",
       "      <td>10.231</td>\n",
       "      <td>1.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.817</td>\n",
       "      <td>12.489</td>\n",
       "      <td>1.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>4096</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.817</td>\n",
       "      <td>11.387</td>\n",
       "      <td>2.459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "80           4      512        2048           4                 0     0.198   \n",
       "81           4      512        2048           4                 1     0.199   \n",
       "82           4      512        2048           5                 0     0.244   \n",
       "83           4      512        2048           5                 1     0.244   \n",
       "84           4      512        4096           3                 0     0.520   \n",
       "85           4      512        4096           3                 1     0.520   \n",
       "86           4      512        4096           4                 0     0.669   \n",
       "87           4      512        4096           4                 1     0.670   \n",
       "88           4      512        4096           5                 0     0.817   \n",
       "89           4      512        4096           5                 1     0.817   \n",
       "\n",
       "    memory  time_bwd  \n",
       "80   5.218     0.401  \n",
       "81   4.803     0.546  \n",
       "82   5.652     0.480  \n",
       "83   5.100     0.654  \n",
       "84   9.627     1.401  \n",
       "85   9.076     1.734  \n",
       "86  11.058     1.556  \n",
       "87  10.231     1.998  \n",
       "88  12.489     1.930  \n",
       "89  11.387     2.459  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.076</td>\n",
       "      <td>3.734</td>\n",
       "      <td>0.166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077</td>\n",
       "      <td>3.454</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.098</td>\n",
       "      <td>3.960</td>\n",
       "      <td>0.203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098</td>\n",
       "      <td>3.540</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.124</td>\n",
       "      <td>4.186</td>\n",
       "      <td>0.239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.123</td>\n",
       "      <td>3.625</td>\n",
       "      <td>0.322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.273</td>\n",
       "      <td>5.423</td>\n",
       "      <td>0.674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.272</td>\n",
       "      <td>4.865</td>\n",
       "      <td>0.841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.357</td>\n",
       "      <td>6.014</td>\n",
       "      <td>0.771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.357</td>\n",
       "      <td>5.177</td>\n",
       "      <td>1.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "90           4     1024        1024           3                 0     0.076   \n",
       "91           4     1024        1024           3                 1     0.077   \n",
       "92           4     1024        1024           4                 0     0.098   \n",
       "93           4     1024        1024           4                 1     0.098   \n",
       "94           4     1024        1024           5                 0     0.124   \n",
       "95           4     1024        1024           5                 1     0.123   \n",
       "96           4     1024        2048           3                 0     0.273   \n",
       "97           4     1024        2048           3                 1     0.272   \n",
       "98           4     1024        2048           4                 0     0.357   \n",
       "99           4     1024        2048           4                 1     0.357   \n",
       "\n",
       "    memory  time_bwd  \n",
       "90   3.734     0.166  \n",
       "91   3.454     0.222  \n",
       "92   3.960     0.203  \n",
       "93   3.540     0.276  \n",
       "94   4.186     0.239  \n",
       "95   3.625     0.322  \n",
       "96   5.423     0.674  \n",
       "97   4.865     0.841  \n",
       "98   6.014     0.771  \n",
       "99   5.177     1.002  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch_size</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>checkpoint_level</th>\n",
       "      <th>time_fwd</th>\n",
       "      <th>memory</th>\n",
       "      <th>time_bwd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.435</td>\n",
       "      <td>6.605</td>\n",
       "      <td>0.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436</td>\n",
       "      <td>5.489</td>\n",
       "      <td>1.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.028</td>\n",
       "      <td>10.898</td>\n",
       "      <td>2.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029</td>\n",
       "      <td>9.793</td>\n",
       "      <td>3.391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.342</td>\n",
       "      <td>12.637</td>\n",
       "      <td>3.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>4</td>\n",
       "      <td>1024</td>\n",
       "      <td>4096</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.341</td>\n",
       "      <td>10.979</td>\n",
       "      <td>3.944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     batch_size  seq_len  hidden_dim  num_layers  checkpoint_level  time_fwd  \\\n",
       "100           4     1024        2048           5                 0     0.435   \n",
       "101           4     1024        2048           5                 1     0.436   \n",
       "102           4     1024        4096           3                 0     1.028   \n",
       "103           4     1024        4096           3                 1     1.029   \n",
       "104           4     1024        4096           4                 0     1.342   \n",
       "105           4     1024        4096           4                 1     1.341   \n",
       "\n",
       "     memory  time_bwd  \n",
       "100   6.605     0.941  \n",
       "101   5.489     1.220  \n",
       "102  10.898     2.731  \n",
       "103   9.793     3.391  \n",
       "104  12.637     3.073  \n",
       "105  10.979     3.944  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for i in range(11):\n",
    "    display(pd.DataFrame(stats).round(3).iloc[10*i:10*i+10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*В этой задаче удалось посмотреть, как разные параметры влияют на скорость и память в зависимости от характера чекпоинтинга (checkpoint_level). Метод очень удобный для огромных моделей: пересчитываем много тензоров, но зато экономим память*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
