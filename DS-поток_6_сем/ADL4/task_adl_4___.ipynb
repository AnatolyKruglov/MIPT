{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qyq8YF-6S8s"
      },
      "source": [
        "# DS-поток, весна 2025\n",
        "## Задание ADL.4\n",
        "### Подходы к оптимизации процесса обучения LLMs.\n",
        "\n",
        "**Правила:**\n",
        "\n",
        "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
        "* Выполненную работу нужно отправить телеграм-боту `@miptstats_ds24_bot`. Для начала работы с ботом каждый раз отправляйте `/start`. Дождитесь подтверждения от бота, что он принял файл. Если подтверждения нет, то что-то не так. **Работы, присланные иным способом, не принимаются.**\n",
        "* Дедлайны см. в боте. После дедлайна работы не принимаются кроме случаев наличия уважительной причины.\n",
        "* Прислать нужно **ноутбук в формате `ipynb`**.\n",
        "* Следите за размером файлов. **Бот не может принимать файлы весом более 20 Мб.** Если файл получается больше, заранее разделите его на несколько.\n",
        "* Выполнять задание необходимо полностью самостоятельно. **При обнаружении списывания все участники списывания получат штраф.**\n",
        "* Решения, размещенные на каких-либо интернет-ресурсах, не принимаются. Кроме того, публикация решения в открытом доступе может быть приравнена к предоставлении возможности списать.\n",
        "* Для выполнения задания используйте этот ноутбук в качестве основы, ничего не удаляя из него. Можно добавлять необходимое количество ячеек.\n",
        "* Комментарии к решению пишите в markdown-ячейках.\n",
        "* Выполнение задания (ход решения, выводы и пр.) должно быть осуществлено на русском языке.\n",
        "* Если код будет не понятен проверяющему, оценка может быть снижена.\n",
        "* Никакой код из данного задания при проверке запускаться не будет. *Если код студента не выполнен, недописан и т.д., то он не оценивается.*\n",
        "* В каждой задаче не забывайте делать **пояснения и выводы**.\n",
        "* **Код из рассказанных на занятиях ноутбуков** можно использовать без ограничений.\n",
        "\n",
        "**Баллы за задание:**\n",
        "\n",
        "* Реализация &mdash; 80 баллов;\n",
        "* Сравнение и анализ &mdash; 70 баллов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU9xjOMyYurQ"
      },
      "source": [
        "## Описание\n",
        "\n",
        "На занятии мы познакомились с различными техниками, которые используются при обучении больших языковых моделей. В этом домашнем задании вам предстоит решить реальную практическую задачу, которая связана с оптимизацией некоторых слоев в трансформере.\n",
        "\n",
        "Вспомним общую идею техники, которая называется Gradient Checkpointing. Идея заключается в том, чтобы на этапе forward'a не запоминать промежуточные активации, необходимые для backward'a, а вычислять их непосредственно на этапе backward'a. Почему это может быть важно? Оказывается, что активации MLP для больших моделей занимают очень много памяти. Сохранить для backward'а все активации, включая слои внимания, просто невозможно. Возникает вопрос: стоит ли сохранять промежуточные активации MLP или же отдать память под активации attention'a? На практике пересчет активаций для MLP оказывается гораздо быстрее, чем пересчет того же attention'a. В итоге мы можем не сохранять активации MLP, экономить достаточно много памяти, а часть освободившейся памяти отдать под активации attention'a и тем самым даже ускорить обучение!\n",
        "\n",
        "## Реализация\n",
        "Сегодня мы не будем работать с полноценным трансформером, а сфокусируемся только на MLP-блоке. Вам предлагается написать модифицированный слой MLP таким образом, чтобы он поддерживал возможность либо сохранять промежуточные активации, либо пересчитывать их на этапе backward'a. Оформить код нужно будет в виде кастомной `torch.autograd.Function`. Хорошая практика заключается в том, чтобы ваш итоговый слой, который наследуется от `torch.nn.Module` просто вызывал функцию с нужными параметрами. Вам нужно **обязательно** ознакомиться с [постом](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html), в котором вы узнаете, как правильно написать кастомную `torch.autograd.Function` функцию и что это вообще такое.\n",
        "\n",
        "\n",
        "Вспомним с лекции, как выглядит модифицированный MLP для современных арихетктур.\n",
        "$$\n",
        "\\text{FFN}_{\\text{SwiGLU}}\\left(x, W, V, U\\right) = \\left(\\text{Swish}_1\\left(xW\\right)\\otimes xV\\right)U = \\left(\\text{SiLU}\\left(xW\\right)\\otimes xV\\right)U,\n",
        "$$\n",
        "где\n",
        "$$\n",
        "\\text{Swish}_1\\left(x\\right)= \\text{SiLU}\\left(x\\right) = x\\sigma\\left(x\\right)\n",
        "$$\n",
        "\n",
        "Такой MLP-блок используется в [LLaMA](https://arxiv.org/abs/2307.09288)-подобных архитектурах. В этом задании будем использовать его. Посмотрим, как выглядит реализация в виде простого `torch.nn.Module`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5fdC5ZZ4tIsg"
      },
      "outputs": [],
      "source": [
        "class SwigluMLP(nn.Module):\n",
        "    def __init__(self, hidden_size, intermediate_size):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
        "        self.V = nn.Linear(hidden_size, intermediate_size, bias=False)\n",
        "        self.U = nn.Linear(intermediate_size, hidden_size, bias=False)\n",
        "        self.act_fn = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.U(self.act_fn(self.W(x)) * self.V(x))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRH0ZKgcxizx"
      },
      "source": [
        "Теперь перейдем к нашей реализации. Обратите внимание, что `MemoryOptimizedSwigluMLPFunction` принимает на вход `checkpoint_level`. Это переменная нужна для реализации следующей логики:\n",
        "* `checkpoint_level == 0` &mdash; никаких оптимизаций не проводится, промежуточные активации просто сохраняются для переиспользования на этапе backward'a через `ctx.save_for_backward(...)`;\n",
        "* `checkpoint_level == 1` &mdash; для backward'a сохраняются только вход `x` и матрицы `W, V, U`, а на этапе backward'a нужные активации просто снова пересчитываются.\n",
        "\n",
        "Вам нужно реализовать методы `forward` и `backward`. Реализация второго потребует от вас посчитать некоторые промежуточные градиенты в матричном виде. Обязательно **выпишите и поясните** получающиеся формулы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPK-tVC_xmgW"
      },
      "outputs": [],
      "source": [
        "class MemoryOptimizedSwigluMLPFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, W, V, U, checkpoint_level):\n",
        "        # x - тензор размера (batch_size, seq_len, hidden_dim)\n",
        "        <...>\n",
        "        output = <...>\n",
        "        # В зависимости от checkpoint_level сохраняем разные тензоры для backward\n",
        "        # используйте ctx.save_for_backward\n",
        "        if checkpoint_level == 0:\n",
        "            <...>\n",
        "        else:\n",
        "            <...>\n",
        "        ctx.checkpoint_level = checkpoint_level\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, W, V, U, *activations = ctx.saved_tensors\n",
        "        # Достанем сохраненные или снова вычислим активации\n",
        "        # в зависимости от ctx.checkpoint_level\n",
        "        <...>\n",
        "        grad_U = <...>\n",
        "        # скорее всего понадобится вычислить промежуточные градиенты\n",
        "        # grad_xV, grad_xW, grad_silu_xW и т.п.\n",
        "        <...>\n",
        "\n",
        "        grad_x = <...>\n",
        "        grad_W = <...>\n",
        "        grad_V = <...>\n",
        "        return grad_x, grad_W, grad_V, grad_U, None\n",
        "\n",
        "# Определим новый класс, реализующий оптимизированный MLP-слой\n",
        "class MemoryOptimizedSwigluMLP(nn.Module):\n",
        "    def __init__(self, hidden_size, intermediate_size, checkpoint_level):\n",
        "        super(MemoryOptimizedSwigluMLP, self).__init__()\n",
        "        self.W = nn.Parameter(<...>)\n",
        "        self.V = nn.Parameter(<...>)\n",
        "        self.U = nn.Parameter(<...>)\n",
        "        self.checkpoint_level = checkpoint_level\n",
        "\n",
        "    def forward(self, x):\n",
        "        return MemoryOptimizedSwigluMLPFunction.apply(\n",
        "            x,\n",
        "            self.W,\n",
        "            self.V,\n",
        "            self.U,\n",
        "            self.checkpoint_level\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaKpdWgt0lqR"
      },
      "source": [
        "Теперь проверим, что реализованный MLP-слой считается верно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-PaLwg0_0qlZ"
      },
      "outputs": [],
      "source": [
        "# Зададим параметры\n",
        "batch_size = 4\n",
        "seq_len = 256\n",
        "hidden_dim = 768\n",
        "\n",
        "dummy_input = torch.randn(batch_size, seq_len, hidden_dim)\n",
        "# Обратите внимание, что intermediate_size кратно больше hidden_dim\n",
        "# Это типичное \"расширение\", характерное для MLP (FFN) слоя в трансформере\n",
        "swiglu_mlp = SwigluMLP(hidden_dim, hidden_dim * 3)\n",
        "optimized_swiglu_mlp = MemoryOptimizedSwigluMLP(hidden_dim, hidden_dim * 3)\n",
        "\n",
        "# Скопируем параметры, чтобы они были одинаковые\n",
        "with torch.no_grad():\n",
        "    optimized_swiglu_mlp.W.data = swiglu_mlp.W.weight.data.t()\n",
        "    optimized_swiglu_mlp.V.data = swiglu_mlp.V.weight.data.t()\n",
        "    optimized_swiglu_mlp.U.data = swiglu_mlp.U.weight.data.t()\n",
        "\n",
        "# Прогоним модель\n",
        "standard_output = swiglu_mlp(dummy_input)\n",
        "optimized_output = optimized_swiglu_mlp(dummy_input)\n",
        "\n",
        "# Проверим выходы слоев на совпадение\n",
        "assert torch.allclose(standard_output, optimized_output, atol=1e-4)\n",
        "\n",
        "standard_output.sum().backward()\n",
        "optimized_output.sum().backward()\n",
        "\n",
        "# Проверим на совпадение градиенты\n",
        "assert torch.allclose(swiglu_mlp.U.weight.grad, optimized_swiglu_mlp.U.grad.t(), atol=1e-4)\n",
        "assert torch.allclose(swiglu_mlp.V.weight.grad, optimized_swiglu_mlp.V.grad.t(), atol=1e-4)\n",
        "assert torch.allclose(swiglu_mlp.W.weight.grad, optimized_swiglu_mlp.W.grad.t(), atol=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytfh5Nzk4F7p"
      },
      "source": [
        "## Сравнение и анализ\n",
        "\n",
        "Сравните время исполнение forward/backward и объем потребляемой памяти в зависимости от значения `checkpoint_level`. Проведите эксперименты для разных значений `batch_size`, `seq_len`, `hidden_dim`. Сделайте запуски в нескольких сетапах. Попробуйте достаточно большие `seq_len=1024` и `hidden_dim=4096`, а также `num_layers=5`. Сделайте выводы.\n",
        "\n",
        "Некоторые советы:\n",
        "* Для отслеживания потребляемой памяти можете воспользоваться `torch.cuda.max_memory_allocated()`. Желательно после каждого шага очищать статистику, используя `torch.cuda.reset_peak_memory_stats()`. Более подробно рекомендуется почитать документацию по [ссылке](https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-management).\n",
        "* Для подсчета времени можно воспользоваться простым ` time.perf_counter()` или же `time.time()`. Однако с подсчетом времени для GPU-операций все немного хитрее. В PyTorch и других библиотеках, работающих с GPU, операции выполняются асинхронно по отношению к коду, исполняемому на CPU. Такой подход позволяет CPU продолжать работу, не ожидая окончания каждой операции на GPU, что способствует повышению общей производительности за счёт параллельной работы CPU и GPU. Что это значит на практике? Вы можете получить завышенные результаты своих измерений, так как замер времени может завершиться до того, как GPU в действительности закончит выполнение операций.\n",
        "Рассмотрим пример кода:\n",
        "```\n",
        "a = torch.rand(10000, 10000, device=\"cuda\")\n",
        "start_time = time.time()\n",
        "b = a @ a\n",
        "elapsed_time = time.time() - start_time\n",
        "```\n",
        "В этом примере, после запуска операции умножения матриц `a @ a`, мы немедленно измеряем время выполнения функции, не дожидаясь её фактического завершения на GPU.\n",
        "Для получения точных измерений времени выполнения операций на GPU необходимо использовать синхронизацию. В PyTorch это можно сделать с помощью функции `torch.cuda.synchronize()`, которая блокирует выполнение кода на CPU до тех пор, пока все запланированные задачи на соответствующем GPU не будут завершены. \\\n",
        "Пример более грамотного кода:\n",
        "```\n",
        "a = torch.rand(10000, 10000, device='cuda')\n",
        "torch.cuda.synchronize() # ждем завершения всех предыдущих операций на GPU\n",
        "start_time = time.time()\n",
        "b = a @ a\n",
        "torch.cuda.synchronize() # cнова синхронизируемся, чтобы убедиться, что операция завершена\n",
        "elapsed_time = time.time() - start_time\n",
        "```\n",
        "* Для подсчета статистики следует сделать несколько проходов forward/backward для одной модели, а полученные результаты просто усреднить. Для более стабильных результатов выполните также разогрев, то есть некоторое количество прогонов модели перед основным измерением. Это важно, т.к. на результаты измерений могут повлиять дополнительные задержки, связанные с инициализацией и загрузкой ресурсов, температурой GPU, а также различные кэши.\n",
        "* Представьте результаты и выводы в информативном виде, хорошо подойдет какая-нибудь табличка. Затраты по памяти лучше всего указать в Гб, а время исполнения в секундах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "nreczZ67-cKo"
      },
      "outputs": [],
      "source": [
        "# используем несколько слоев, чтобы увидеть выигрыш по памяти\n",
        "# в случае chechkpoint_level == 0 для вычислений очередного слоя будет использована та память, \n",
        "# что осталась для предыдущего\n",
        "# если же checkpoint_level == 1, то придется хранить активации для всех слоев\n",
        "\n",
        "class TestTransformer(nn.Module):\n",
        "    def __init__(self, num_layers, hidden_size, intermediate_size, checkpoint_level):\n",
        "        super(TestTransformer, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            MemoryOptimizedSwigluMLP(hidden_size, intermediate_size, checkpoint_level)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Реализация функции для бенчмарка."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3qiIeNw-4xE"
      },
      "outputs": [],
      "source": [
        "def benchmark_transformer(model, batch_size, seq_len, hidden_dim, num_warmup_steps, num_steps):\n",
        "    statistics = defaultdict(list)\n",
        "\n",
        "    # Генерация входных данных\n",
        "    input_tensor = ...\n",
        "\n",
        "    # Warmup GPU чтобы избежать заниженного перфоманса\n",
        "    for _ in range(num_warmup_steps):\n",
        "        ...\n",
        "\n",
        "    # Основное измерение\n",
        "    for _ in range(num_steps):\n",
        "        ...\n",
        "\n",
        "\n",
        "    return statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uZmZUcCwt8j"
      },
      "outputs": [],
      "source": [
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Вывод:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "..."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
